{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HOkOefftqyg"
   },
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('mnist_train_small .csv','rb'),delimiter=',')\n",
    "\n",
    "# don't need labels!\n",
    "data = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor( dataNorm ).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK8Opkhgp0bO"
   },
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "JK3OO3tAtZkA"
   },
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTAE( n_encode, n_bottleneck):\n",
    "\n",
    "  class aenet(nn.Module):\n",
    "    def __init__(self, n_encode, n_bottleneck):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,n_encode)\n",
    "      \n",
    "      ### encoder layer\n",
    "      self.enc = nn.Linear(n_encode,n_bottleneck)\n",
    "\n",
    "      ### latent layer\n",
    "      self.lat = nn.Linear(n_bottleneck,n_encode)\n",
    "\n",
    "      ### decoder layer\n",
    "      self.dec = nn.Linear(n_encode,784)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.enc(x) )\n",
    "      x = F.relu( self.lat(x) )\n",
    "      y = torch.sigmoid( self.dec(x) )\n",
    "      return y\n",
    "  \n",
    "  # create the model instance\n",
    "  net = aenet(n_encode,n_bottleneck)\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.MSELoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvfGQIRGp0ht"
   },
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "IblJo1NCp0kl"
   },
   "outputs": [],
   "source": [
    "def function2trainTheModel(net, lossfun, optimizer):\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 10000\n",
    "  \n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs)\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # select a random set of images\n",
    "    randomidx = np.random.choice(dataT.shape[0],size=32)\n",
    "    X = dataT[randomidx,:]\n",
    "\n",
    "    # forward pass and loss\n",
    "    yHat = net(X)\n",
    "    loss = lossfun(yHat,X)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # losses in this epoch\n",
    "    losses[epochi] = loss.item()\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return losses,net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpGm9xdQ27Ob"
   },
   "source": [
    "# Parametric experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model parameters\n",
    "n_encode = np.linspace(10, 500, 12, dtype=int)\n",
    "n_bottleneck = np.linspace(5, 100, 8, dtype=int)\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished testing with n_encode=10 and n_bottleneck=5, Average Loss = 0.0461\n",
      "Finished testing with n_encode=10 and n_bottleneck=18, Average Loss = 0.0450\n",
      "Finished testing with n_encode=10 and n_bottleneck=32, Average Loss = 0.0383\n",
      "Finished testing with n_encode=10 and n_bottleneck=45, Average Loss = 0.0457\n",
      "Finished testing with n_encode=10 and n_bottleneck=59, Average Loss = 0.0356\n",
      "Finished testing with n_encode=10 and n_bottleneck=72, Average Loss = 0.0396\n",
      "Finished testing with n_encode=10 and n_bottleneck=86, Average Loss = 0.0377\n",
      "Finished testing with n_encode=10 and n_bottleneck=100, Average Loss = 0.0430\n",
      "Finished testing with n_encode=54 and n_bottleneck=5, Average Loss = 0.0372\n",
      "Finished testing with n_encode=54 and n_bottleneck=18, Average Loss = 0.0152\n",
      "Finished testing with n_encode=54 and n_bottleneck=32, Average Loss = 0.0133\n",
      "Finished testing with n_encode=54 and n_bottleneck=45, Average Loss = 0.0119\n",
      "Finished testing with n_encode=54 and n_bottleneck=59, Average Loss = 0.0113\n",
      "Finished testing with n_encode=54 and n_bottleneck=72, Average Loss = 0.0109\n",
      "Finished testing with n_encode=54 and n_bottleneck=86, Average Loss = 0.0112\n",
      "Finished testing with n_encode=54 and n_bottleneck=100, Average Loss = 0.0111\n",
      "Finished testing with n_encode=99 and n_bottleneck=5, Average Loss = 0.0304\n",
      "Finished testing with n_encode=99 and n_bottleneck=18, Average Loss = 0.0165\n",
      "Finished testing with n_encode=99 and n_bottleneck=32, Average Loss = 0.0114\n",
      "Finished testing with n_encode=99 and n_bottleneck=45, Average Loss = 0.0098\n",
      "Finished testing with n_encode=99 and n_bottleneck=59, Average Loss = 0.0076\n",
      "Finished testing with n_encode=99 and n_bottleneck=72, Average Loss = 0.0081\n",
      "Finished testing with n_encode=99 and n_bottleneck=86, Average Loss = 0.0070\n",
      "Finished testing with n_encode=99 and n_bottleneck=100, Average Loss = 0.0066\n",
      "Finished testing with n_encode=143 and n_bottleneck=5, Average Loss = 0.0392\n",
      "Finished testing with n_encode=143 and n_bottleneck=18, Average Loss = 0.0199\n",
      "Finished testing with n_encode=143 and n_bottleneck=32, Average Loss = 0.0084\n",
      "Finished testing with n_encode=143 and n_bottleneck=45, Average Loss = 0.0082\n",
      "Finished testing with n_encode=143 and n_bottleneck=59, Average Loss = 0.0065\n",
      "Finished testing with n_encode=143 and n_bottleneck=72, Average Loss = 0.0063\n",
      "Finished testing with n_encode=143 and n_bottleneck=86, Average Loss = 0.0057\n",
      "Finished testing with n_encode=143 and n_bottleneck=100, Average Loss = 0.0059\n",
      "Finished testing with n_encode=188 and n_bottleneck=5, Average Loss = 0.0283\n",
      "Finished testing with n_encode=188 and n_bottleneck=18, Average Loss = 0.0177\n",
      "Finished testing with n_encode=188 and n_bottleneck=32, Average Loss = 0.0084\n",
      "Finished testing with n_encode=188 and n_bottleneck=45, Average Loss = 0.0080\n",
      "Finished testing with n_encode=188 and n_bottleneck=59, Average Loss = 0.0063\n",
      "Finished testing with n_encode=188 and n_bottleneck=72, Average Loss = 0.0052\n",
      "Finished testing with n_encode=188 and n_bottleneck=86, Average Loss = 0.0050\n",
      "Finished testing with n_encode=188 and n_bottleneck=100, Average Loss = 0.0043\n",
      "Finished testing with n_encode=232 and n_bottleneck=5, Average Loss = 0.0338\n",
      "Finished testing with n_encode=232 and n_bottleneck=18, Average Loss = 0.0163\n",
      "Finished testing with n_encode=232 and n_bottleneck=32, Average Loss = 0.0099\n",
      "Finished testing with n_encode=232 and n_bottleneck=45, Average Loss = 0.0075\n",
      "Finished testing with n_encode=232 and n_bottleneck=59, Average Loss = 0.0058\n",
      "Finished testing with n_encode=232 and n_bottleneck=72, Average Loss = 0.0059\n",
      "Finished testing with n_encode=232 and n_bottleneck=86, Average Loss = 0.0046\n",
      "Finished testing with n_encode=232 and n_bottleneck=100, Average Loss = 0.0044\n",
      "Finished testing with n_encode=277 and n_bottleneck=5, Average Loss = 0.0301\n",
      "Finished testing with n_encode=277 and n_bottleneck=18, Average Loss = 0.0190\n",
      "Finished testing with n_encode=277 and n_bottleneck=32, Average Loss = 0.0090\n",
      "Finished testing with n_encode=277 and n_bottleneck=45, Average Loss = 0.0084\n",
      "Finished testing with n_encode=277 and n_bottleneck=59, Average Loss = 0.0051\n",
      "Finished testing with n_encode=277 and n_bottleneck=72, Average Loss = 0.0045\n",
      "Finished testing with n_encode=277 and n_bottleneck=86, Average Loss = 0.0047\n",
      "Finished testing with n_encode=277 and n_bottleneck=100, Average Loss = 0.0042\n",
      "Finished testing with n_encode=321 and n_bottleneck=5, Average Loss = 0.0373\n",
      "Finished testing with n_encode=321 and n_bottleneck=18, Average Loss = 0.0155\n",
      "Finished testing with n_encode=321 and n_bottleneck=32, Average Loss = 0.0156\n",
      "Finished testing with n_encode=321 and n_bottleneck=45, Average Loss = 0.0080\n",
      "Finished testing with n_encode=321 and n_bottleneck=59, Average Loss = 0.0051\n",
      "Finished testing with n_encode=321 and n_bottleneck=72, Average Loss = 0.0048\n",
      "Finished testing with n_encode=321 and n_bottleneck=86, Average Loss = 0.0048\n",
      "Finished testing with n_encode=321 and n_bottleneck=100, Average Loss = 0.0039\n",
      "Finished testing with n_encode=366 and n_bottleneck=5, Average Loss = 0.0329\n",
      "Finished testing with n_encode=366 and n_bottleneck=18, Average Loss = 0.0172\n",
      "Finished testing with n_encode=366 and n_bottleneck=32, Average Loss = 0.0116\n",
      "Finished testing with n_encode=366 and n_bottleneck=45, Average Loss = 0.0059\n",
      "Finished testing with n_encode=366 and n_bottleneck=59, Average Loss = 0.0050\n",
      "Finished testing with n_encode=366 and n_bottleneck=72, Average Loss = 0.0047\n",
      "Finished testing with n_encode=366 and n_bottleneck=86, Average Loss = 0.0041\n",
      "Finished testing with n_encode=366 and n_bottleneck=100, Average Loss = 0.0043\n",
      "Finished testing with n_encode=410 and n_bottleneck=5, Average Loss = 0.0315\n",
      "Finished testing with n_encode=410 and n_bottleneck=18, Average Loss = 0.0189\n",
      "Finished testing with n_encode=410 and n_bottleneck=32, Average Loss = 0.0084\n",
      "Finished testing with n_encode=410 and n_bottleneck=45, Average Loss = 0.0058\n",
      "Finished testing with n_encode=410 and n_bottleneck=59, Average Loss = 0.0051\n",
      "Finished testing with n_encode=410 and n_bottleneck=72, Average Loss = 0.0050\n",
      "Finished testing with n_encode=410 and n_bottleneck=86, Average Loss = 0.0038\n",
      "Finished testing with n_encode=410 and n_bottleneck=100, Average Loss = 0.0036\n",
      "Finished testing with n_encode=455 and n_bottleneck=5, Average Loss = 0.0358\n",
      "Finished testing with n_encode=455 and n_bottleneck=18, Average Loss = 0.0178\n",
      "Finished testing with n_encode=455 and n_bottleneck=32, Average Loss = 0.0085\n",
      "Finished testing with n_encode=455 and n_bottleneck=45, Average Loss = 0.0063\n",
      "Finished testing with n_encode=455 and n_bottleneck=59, Average Loss = 0.0049\n",
      "Finished testing with n_encode=455 and n_bottleneck=72, Average Loss = 0.0041\n",
      "Finished testing with n_encode=455 and n_bottleneck=86, Average Loss = 0.0045\n",
      "Finished testing with n_encode=455 and n_bottleneck=100, Average Loss = 0.0039\n",
      "Finished testing with n_encode=500 and n_bottleneck=5, Average Loss = 0.0338\n",
      "Finished testing with n_encode=500 and n_bottleneck=18, Average Loss = 0.0153\n",
      "Finished testing with n_encode=500 and n_bottleneck=32, Average Loss = 0.0069\n",
      "Finished testing with n_encode=500 and n_bottleneck=45, Average Loss = 0.0078\n",
      "Finished testing with n_encode=500 and n_bottleneck=59, Average Loss = 0.0047\n",
      "Finished testing with n_encode=500 and n_bottleneck=72, Average Loss = 0.0035\n",
      "Finished testing with n_encode=500 and n_bottleneck=86, Average Loss = 0.0041\n",
      "Finished testing with n_encode=500 and n_bottleneck=100, Average Loss = 0.0039\n",
      "\n",
      "Average Loss for all combinations:\n",
      "n_encode=10, n_bottleneck=5: Average Loss = 0.0461\n",
      "n_encode=10, n_bottleneck=18: Average Loss = 0.0450\n",
      "n_encode=10, n_bottleneck=32: Average Loss = 0.0383\n",
      "n_encode=10, n_bottleneck=45: Average Loss = 0.0457\n",
      "n_encode=10, n_bottleneck=59: Average Loss = 0.0356\n",
      "n_encode=10, n_bottleneck=72: Average Loss = 0.0396\n",
      "n_encode=10, n_bottleneck=86: Average Loss = 0.0377\n",
      "n_encode=10, n_bottleneck=100: Average Loss = 0.0430\n",
      "n_encode=54, n_bottleneck=5: Average Loss = 0.0372\n",
      "n_encode=54, n_bottleneck=18: Average Loss = 0.0152\n",
      "n_encode=54, n_bottleneck=32: Average Loss = 0.0133\n",
      "n_encode=54, n_bottleneck=45: Average Loss = 0.0119\n",
      "n_encode=54, n_bottleneck=59: Average Loss = 0.0113\n",
      "n_encode=54, n_bottleneck=72: Average Loss = 0.0109\n",
      "n_encode=54, n_bottleneck=86: Average Loss = 0.0112\n",
      "n_encode=54, n_bottleneck=100: Average Loss = 0.0111\n",
      "n_encode=99, n_bottleneck=5: Average Loss = 0.0304\n",
      "n_encode=99, n_bottleneck=18: Average Loss = 0.0165\n",
      "n_encode=99, n_bottleneck=32: Average Loss = 0.0114\n",
      "n_encode=99, n_bottleneck=45: Average Loss = 0.0098\n",
      "n_encode=99, n_bottleneck=59: Average Loss = 0.0076\n",
      "n_encode=99, n_bottleneck=72: Average Loss = 0.0081\n",
      "n_encode=99, n_bottleneck=86: Average Loss = 0.0070\n",
      "n_encode=99, n_bottleneck=100: Average Loss = 0.0066\n",
      "n_encode=143, n_bottleneck=5: Average Loss = 0.0392\n",
      "n_encode=143, n_bottleneck=18: Average Loss = 0.0199\n",
      "n_encode=143, n_bottleneck=32: Average Loss = 0.0084\n",
      "n_encode=143, n_bottleneck=45: Average Loss = 0.0082\n",
      "n_encode=143, n_bottleneck=59: Average Loss = 0.0065\n",
      "n_encode=143, n_bottleneck=72: Average Loss = 0.0063\n",
      "n_encode=143, n_bottleneck=86: Average Loss = 0.0057\n",
      "n_encode=143, n_bottleneck=100: Average Loss = 0.0059\n",
      "n_encode=188, n_bottleneck=5: Average Loss = 0.0283\n",
      "n_encode=188, n_bottleneck=18: Average Loss = 0.0177\n",
      "n_encode=188, n_bottleneck=32: Average Loss = 0.0084\n",
      "n_encode=188, n_bottleneck=45: Average Loss = 0.0080\n",
      "n_encode=188, n_bottleneck=59: Average Loss = 0.0063\n",
      "n_encode=188, n_bottleneck=72: Average Loss = 0.0052\n",
      "n_encode=188, n_bottleneck=86: Average Loss = 0.0050\n",
      "n_encode=188, n_bottleneck=100: Average Loss = 0.0043\n",
      "n_encode=232, n_bottleneck=5: Average Loss = 0.0338\n",
      "n_encode=232, n_bottleneck=18: Average Loss = 0.0163\n",
      "n_encode=232, n_bottleneck=32: Average Loss = 0.0099\n",
      "n_encode=232, n_bottleneck=45: Average Loss = 0.0075\n",
      "n_encode=232, n_bottleneck=59: Average Loss = 0.0058\n",
      "n_encode=232, n_bottleneck=72: Average Loss = 0.0059\n",
      "n_encode=232, n_bottleneck=86: Average Loss = 0.0046\n",
      "n_encode=232, n_bottleneck=100: Average Loss = 0.0044\n",
      "n_encode=277, n_bottleneck=5: Average Loss = 0.0301\n",
      "n_encode=277, n_bottleneck=18: Average Loss = 0.0190\n",
      "n_encode=277, n_bottleneck=32: Average Loss = 0.0090\n",
      "n_encode=277, n_bottleneck=45: Average Loss = 0.0084\n",
      "n_encode=277, n_bottleneck=59: Average Loss = 0.0051\n",
      "n_encode=277, n_bottleneck=72: Average Loss = 0.0045\n",
      "n_encode=277, n_bottleneck=86: Average Loss = 0.0047\n",
      "n_encode=277, n_bottleneck=100: Average Loss = 0.0042\n",
      "n_encode=321, n_bottleneck=5: Average Loss = 0.0373\n",
      "n_encode=321, n_bottleneck=18: Average Loss = 0.0155\n",
      "n_encode=321, n_bottleneck=32: Average Loss = 0.0156\n",
      "n_encode=321, n_bottleneck=45: Average Loss = 0.0080\n",
      "n_encode=321, n_bottleneck=59: Average Loss = 0.0051\n",
      "n_encode=321, n_bottleneck=72: Average Loss = 0.0048\n",
      "n_encode=321, n_bottleneck=86: Average Loss = 0.0048\n",
      "n_encode=321, n_bottleneck=100: Average Loss = 0.0039\n",
      "n_encode=366, n_bottleneck=5: Average Loss = 0.0329\n",
      "n_encode=366, n_bottleneck=18: Average Loss = 0.0172\n",
      "n_encode=366, n_bottleneck=32: Average Loss = 0.0116\n",
      "n_encode=366, n_bottleneck=45: Average Loss = 0.0059\n",
      "n_encode=366, n_bottleneck=59: Average Loss = 0.0050\n",
      "n_encode=366, n_bottleneck=72: Average Loss = 0.0047\n",
      "n_encode=366, n_bottleneck=86: Average Loss = 0.0041\n",
      "n_encode=366, n_bottleneck=100: Average Loss = 0.0043\n",
      "n_encode=410, n_bottleneck=5: Average Loss = 0.0315\n",
      "n_encode=410, n_bottleneck=18: Average Loss = 0.0189\n",
      "n_encode=410, n_bottleneck=32: Average Loss = 0.0084\n",
      "n_encode=410, n_bottleneck=45: Average Loss = 0.0058\n",
      "n_encode=410, n_bottleneck=59: Average Loss = 0.0051\n",
      "n_encode=410, n_bottleneck=72: Average Loss = 0.0050\n",
      "n_encode=410, n_bottleneck=86: Average Loss = 0.0038\n",
      "n_encode=410, n_bottleneck=100: Average Loss = 0.0036\n",
      "n_encode=455, n_bottleneck=5: Average Loss = 0.0358\n",
      "n_encode=455, n_bottleneck=18: Average Loss = 0.0178\n",
      "n_encode=455, n_bottleneck=32: Average Loss = 0.0085\n",
      "n_encode=455, n_bottleneck=45: Average Loss = 0.0063\n",
      "n_encode=455, n_bottleneck=59: Average Loss = 0.0049\n",
      "n_encode=455, n_bottleneck=72: Average Loss = 0.0041\n",
      "n_encode=455, n_bottleneck=86: Average Loss = 0.0045\n",
      "n_encode=455, n_bottleneck=100: Average Loss = 0.0039\n",
      "n_encode=500, n_bottleneck=5: Average Loss = 0.0338\n",
      "n_encode=500, n_bottleneck=18: Average Loss = 0.0153\n",
      "n_encode=500, n_bottleneck=32: Average Loss = 0.0069\n",
      "n_encode=500, n_bottleneck=45: Average Loss = 0.0078\n",
      "n_encode=500, n_bottleneck=59: Average Loss = 0.0047\n",
      "n_encode=500, n_bottleneck=72: Average Loss = 0.0035\n",
      "n_encode=500, n_bottleneck=86: Average Loss = 0.0041\n",
      "n_encode=500, n_bottleneck=100: Average Loss = 0.0039\n"
     ]
    }
   ],
   "source": [
    "# start the experiment!\n",
    "for encode_idx, current_n_encode in enumerate(n_encode):\n",
    "    for bottleneck_idx, current_n_bottleneck in enumerate(n_bottleneck):\n",
    "\n",
    "        # create and train a fresh model\n",
    "        net, lossfun, optimizer = createTheMNISTAE(current_n_encode, current_n_bottleneck)\n",
    "        losses, _ = function2trainTheModel(net, lossfun, optimizer)\n",
    "\n",
    "        # store the average of the last 3 losses\n",
    "        if len(losses) >= 3:\n",
    "            avg_loss = torch.mean(losses[-3:]).item()\n",
    "        else:\n",
    "            avg_loss = torch.mean(losses).item()\n",
    "\n",
    "        results[(current_n_encode, current_n_bottleneck)] = {'average_loss': avg_loss}\n",
    "\n",
    "        # print a friendly status message\n",
    "        print(f'Finished testing with n_encode={current_n_encode} and n_bottleneck={current_n_bottleneck}, Average Loss = {avg_loss:.4f}')\n",
    "\n",
    "# You can now analyze the 'results' dictionary\n",
    "print(\"\\nAverage Loss for all combinations:\")\n",
    "for params, res in results.items():\n",
    "    print(f\"n_encode={params[0]}, n_bottleneck={params[1]}: Average Loss = {res['average_loss']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMYBOs+CJWj6NXJ0M+a+1O",
   "collapsed_sections": [],
   "name": "DUDL_autoenc_denoisingMNIST.ipynb",
   "provenance": [
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1619155961717
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
