{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0491d3bd-f726-4839-9980-9eb3e4d2e018",
   "metadata": {},
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: ANNs\n",
    "### LECTURE: ANN for regression\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1282f1d-c67f-47fd-8c26-b32bdb333539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd256b10-4c74-4b13-a227-2987a5884401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) How much data is \"enough\"? Try different values of N and see how low the loss gets. \n",
    "#    Do you still get low loss (\"low\" is subjective, but let's say loss<.25) with N=10? N=5?\n",
    "\n",
    "## create data -----------------------\n",
    "\n",
    "N = 5\n",
    "x = torch.randn(N,1)\n",
    "y = x + torch.randn(N,1)/2\n",
    "\n",
    "# build model\n",
    "ANNreg = nn.Sequential(\n",
    "    nn.Linear(1,1), # input Layer\n",
    "    nn.ReLU(),      # Activation function\n",
    "    nn.Linear(1,1)  # output layer \n",
    ")\n",
    "\n",
    "##PARAMETERS ------------------\n",
    "# learning rate\n",
    "learningRate = .05\n",
    "\n",
    "# loss function\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "# optimizer (the flavor of gradient descent to implement)\n",
    "optimizer = torch.optim.SGD(ANNreg.parameters(),lr=learningRate)\n",
    "\n",
    "# train the model!-------------------------------\n",
    "numepochs = 500\n",
    "losses = torch.zeros(numepochs)\n",
    "\n",
    "## Train the model\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # forward pass\n",
    "  yHat = ANNreg(x)\n",
    "\n",
    "  # compute loss\n",
    "  loss = lossfun(yHat,y)\n",
    "  losses[epochi] = loss\n",
    "\n",
    "  # backprop\n",
    "  optimizer.zero_grad() # Réinitialiser les gradients\n",
    "  loss.backward() # Calculer les nouveaux gradients\n",
    "  optimizer.step() # Mettre à jour les paramètres du modèle en fonction des gradients\n",
    "\n",
    "# show the losses-------------------\n",
    "\n",
    "# manually compute losses\n",
    "# final forward pass\n",
    "predictions = ANNreg(x)\n",
    "\n",
    "# final loss (MSE)\n",
    "testloss = (predictions-y).pow(2).mean()\n",
    "\n",
    "plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n",
    "plt.plot(numepochs,testloss.detach(),'ro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Final loss = %g' %testloss.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1ae18-c83e-4e5d-b415-58175e3bf236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Does your conclusion above depend on the amount of noise in the data? Try changing the noise level\n",
    "#    by changing the division (\"/2\") when creating y as x+randn.\n",
    "## create data -----------------------\n",
    "\n",
    "N = 5\n",
    "x = torch.randn(N,1)\n",
    "y = x + torch.randn(N,1) * 2\n",
    "\n",
    "# build model\n",
    "ANNreg = nn.Sequential(\n",
    "    nn.Linear(1,1), # input Layer\n",
    "    nn.ReLU(),      # Activation function\n",
    "    nn.Linear(1,1)  # output layer \n",
    ")\n",
    "\n",
    "##PARAMETERS ------------------\n",
    "# learning rate\n",
    "learningRate = .05\n",
    "\n",
    "# loss function\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "# optimizer (the flavor of gradient descent to implement)\n",
    "optimizer = torch.optim.SGD(ANNreg.parameters(),lr=learningRate)\n",
    "\n",
    "# train the model!-------------------------------\n",
    "numepochs = 500\n",
    "losses = torch.zeros(numepochs)\n",
    "\n",
    "## Train the model\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # forward pass\n",
    "  yHat = ANNreg(x)\n",
    "\n",
    "  # compute loss\n",
    "  loss = lossfun(yHat,y)\n",
    "  losses[epochi] = loss\n",
    "\n",
    "  # backprop\n",
    "  optimizer.zero_grad() # Réinitialiser les gradients\n",
    "  loss.backward() # Calculer les nouveaux gradients\n",
    "  optimizer.step() # Mettre à jour les paramètres du modèle en fonction des gradients\n",
    "\n",
    "# show the losses-------------------\n",
    "\n",
    "# manually compute losses\n",
    "# final forward pass\n",
    "predictions = ANNreg(x)\n",
    "\n",
    "# final loss (MSE)\n",
    "testloss = (predictions-y).pow(2).mean()\n",
    "\n",
    "plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n",
    "plt.plot(numepochs,testloss.detach(),'ro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Final loss = %g' %testloss.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abad274-0a9d-4f94-989f-a0a4199b9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Notice that the model doesn't always work well. Put the original code (that is, N=30 and /2 noise)\n",
    "#    into a function or a for-loop and repeat the training 100 times (each time using a fresh model instance).\n",
    "#    Then count the number of times the model had a loss>.25.\n",
    "\n",
    "## create data -----------------------\n",
    "\n",
    "N = 30\n",
    "x = torch.randn(N, 1)\n",
    "y = x + torch.randn(N, 1) / 2\n",
    "\n",
    "lossSup = 0.25\n",
    "lossSum = 0\n",
    "nTest = 100\n",
    "\n",
    "for _ in range(nTest):\n",
    "\n",
    "    # build model\n",
    "    ANNreg = nn.Sequential(\n",
    "        nn.Linear(1, 1),  # input Layer\n",
    "        nn.ReLU(),  # Activation function\n",
    "        nn.Linear(1, 1)  # output layer\n",
    "    )\n",
    "\n",
    "    ## PARAMETERS ------------------\n",
    "    # learning rate\n",
    "    learningRate = .05\n",
    "\n",
    "    # loss function\n",
    "    lossfun = nn.MSELoss()\n",
    "\n",
    "    # optimizer (the flavor of gradient descent to implement)\n",
    "    optimizer = torch.optim.SGD(ANNreg.parameters(), lr=learningRate)\n",
    "\n",
    "    # train the model!-------------------------------\n",
    "    numepochs = 500\n",
    "    losses = torch.zeros(numepochs)\n",
    "\n",
    "    ## Train the model\n",
    "    for epochi in range(numepochs):\n",
    "\n",
    "        # forward pass\n",
    "        yHat = ANNreg(x)\n",
    "\n",
    "        # compute loss\n",
    "        loss = lossfun(yHat, y)\n",
    "        losses[epochi] = loss\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()  # Réinitialiser les gradients\n",
    "        loss.backward()  # Calculer les nouveaux gradients\n",
    "        optimizer.step()  # Mettre à jour les paramètres du modèle en fonction des gradients\n",
    "\n",
    "    # Get the final loss after training.\n",
    "    final_loss = losses[-1]\n",
    "\n",
    "    if final_loss.item() > lossSup:\n",
    "        lossSum += 1\n",
    "\n",
    "print(f\"Nombre de pertes supérieures à {lossSup}: {lossSum}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
